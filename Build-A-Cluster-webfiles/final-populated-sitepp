# Set path for Exec so we don't need full paths
Exec { path => [ "/bin/", "/sbin/" , "/usr/bin/", "/usr/sbin/" ] }

# set IP addresses for all machines in the cluster
$headnodeip='172.31.17.193'
$storagenodeip='172.31.17.195'
$computeoneip='172.31.17.194'
$computetwoip='172.31.21.170'

class base_cluster {

  # Create swap memory
  # Defaults to creating virtual memory that is the same size as physicali
  # memory
  include swap_file

  # Turn down selinux so it won't bother us
  class { 'selinux':
    mode => 'permissive',
  }

  # Misc package installs
  package { 'bind-utils':
    ensure => present,
  }

  package { 'wget':
    ensure => present,
  }

  package { 'lsof':
    ensure => present,
  }

  package { 'mlocate':
    ensure => present,
  }

  package { 'strace':
    ensure => present,
  }

  package { 'telnet':
    ensure => present,
  }

  package { 'nc':
    ensure => present,
  }

  package { 'screen':
    ensure => present,
  }

  # Firewall for machines in the cluster
  resources { "firewall":
    purge => true
  }

  class { 'my_fw': }
  class { 'firewall': }

  # allow the all of the nodes to connect to each other
  # this will open every machine's firewall up to the others
  firewall { '003 INPUT allow head ip':
      chain => 'INPUT',
      action => 'accept',
      proto => 'all',
      source => "${headnodeip}/32",
  }

  firewall { '004 INPUT allow storage ip':
      chain => 'INPUT',
      action => 'accept',
      proto => 'all',
      source => "${storagenodeip}/32",
  }

  firewall { '005 INPUT allow compute1 ip':
      chain => 'INPUT',
      action => 'accept',
      proto => 'all',
      source => "${computeoneip}/32",
  }

  firewall { '006 INPUT allow compute2 ip':
      chain => 'INPUT',
      action => 'accept',
      proto => 'all',
      source => "${computetwoip}/32",
  }

  # order of operations to prevent "no dns" race condition
  Class['dnsmasq'] -> Class['resolv_conf'] -> Exec['set-hostname-to-dns']

  # install and setup dnsmasq
  class { 'dnsmasq':
    interface         => 'lo',
    listen_address    => '127.0.0.1',
    domain            => 'cluster',
    port              => '53',
    expand_hosts      => false,
    enable_tftp       => false,
    domain_needed     => true,
    bogus_priv        => true,
    no_negcache       => true,
    no_hosts          => true,
    cache_size        => 5000,
  }

  # set dnsmasq server to Google's public dns server
  dnsmasq::dnsserver { 'dns':
    ip => '8.8.8.8',
  }

  # forward and reverse for the cluster
  dnsmasq::address { "head.cluster":
    ip  => $headnodeip,
  }

  dnsmasq::ptr { "193.17.31.172.in-addr.arpa.":
    value  => 'head.cluster',
  }

  dnsmasq::address { "storage.cluster":
    ip  => $storagenodeip,
  }

  dnsmasq::ptr { "195.17.31.172.in-addr.arpa.":
    value  => 'storage.cluster',
  }

  dnsmasq::address { "compute1.cluster":
    ip  => $computeoneip,
  }

  dnsmasq::ptr { "194.17.31.172.in-addr.arpa.":
    value  => 'compute1.cluster',
  }

  dnsmasq::address { "compute2.cluster":
    ip  => $computetwoip,
  }

  dnsmasq::ptr { "170.21.31.172.in-addr.arpa.":
    value  => 'compute2.cluster',
  }

  # setup resolv.conf to point to dnsmasq instance
  class { 'resolv_conf':
    nameservers => ['127.0.0.1'],
    searchpath  => ['cluster'],
  }

  # reset the hostname using our new dnsmasq names
  exec { "set-hostname-to-dns":
    # this sets the hostname to the dig name of the ip address on eth0 and without the period at the end
    command => 'hostname $(dig +short -x `hostname -I` | sed "s/\.\+$//")',
  }

  # Create /apps/ directory for shared application space
  file { "/apps":
    ensure => "directory",
  }

  # add an account for running jobs
  account { 
    'sharrell':
      home_dir => '/home/sharrell',
      groups   => [ 'wheel', 'users' ],
      comment   => 'Stephen Lien Harrell',
      uid => 500,
  }

  # Install torque libs
  package { 'libxml2':
    ensure => present,
  }

  package { 'torque':
    ensure => 'installed',
    source => 'http://web.rcac.purdue.edu/~sharrell/buildacluster/torque-4.1.7-1.adaptive.el6.x86_64.rpm',
    provider => 'rpm',
  }

  # Install environment modules and configure modules for openmpi and openblas
  package { 'environment-modules':
    ensure => present,
  }

  package { 'gcc-c++':
    ensure => present,
  }

  package { 'gcc-gfortran':
    ensure => present,
  }

  file { "/usr/share/Modules/modulefiles/openblas":
    ensure => "directory"
  }

  file { "/usr/share/Modules/modulefiles/openblas/.version":
    ensure => "present",
    content => "#%Module1.0
set ModulesVersion \"0.2.10\""
  }

  file { "/usr/share/Modules/modulefiles/openblas/0.2.10":
    ensure => "present",
    content => "#%Module1.0
module-whatis   \"invoke openblas-0.2.10\"
set             version         0.2.10
set             app             openblas
set             modroot         /apps/openblas-0.2.10/
prepend-path    PATH            \$modroot/bin
prepend-path    LD_LIBRARY_PATH \$modroot/lib"
  }

  file { "/usr/share/Modules/modulefiles/openmpi":
    ensure => "directory"
  }

  file { "/usr/share/Modules/modulefiles/openmpi/.version":
    ensure => "present",
    content => "#%Module1.0
set ModulesVersion \"1.7.5\""
  }

  file { "/usr/share/Modules/modulefiles/openmpi/1.7.5":
    ensure => "present",
    content => "#%Module1.0
module-whatis   \"invoke openmpi-1.7.5\"
set             version         1.7.5
set             app             openmpi
set             modroot         /apps/openmpi-1.7.5/
prepend-path    PATH            \$modroot/bin
prepend-path    LD_LIBRARY_PATH \$modroot/lib
setenv          MPI_HOME        \$modroot
setenv          CC              mpicc
setenv          CXX             mpiCC
setenv          F77             mpif77
setenv          FC              mpif90\n"
  }

  # Set all machines to log to head.cluster
  class { 'rsyslog::client':
    remote_type    => 'tcp',
    server         => 'head.cluster',
  }

}

class head_node {

  # setup apache module
  class { 'apache':
    default_confd_files => false,
    purge_configs => false,
  }
  class { 'apache::mod::dav_svn': }

  # configure subversion vhost
  apache::vhost { 'headnode.internal':
    port => 443,
    docroot => '/var/www/html/',
    ssl => true,
    ssl_cert => '/etc/httpd/ssl/apache.crt',  
    ssl_key  => '/etc/httpd/ssl/apache.key',
    custom_fragment => '
      <Location /puppet >
        AuthType Basic
        AuthName "Puppet Cluster Repository"
        AuthUserFile "/etc/httpd/auth_user_file"
        Require valid-user
        DAV svn
        SVNPath /var/svn/puppet/
      </Location>'
  }

  # allow anyone to be able to access https (puppet svn)
  firewall { '100 allow https access':
    state => ['NEW'],
    dport   => 443,
    proto  => tcp,
    action => accept,
  }

  # mounts for home and apps NFS share
  mounts { 'storage server home':
    ensure => present,
    source => "${storagenodeip}:/home",
    dest   => '/home',
    type   => 'nfs',
    opts   => 'nofail,defaults,vers=3,rw,noatime',
  }

  mounts { 'storage server apps':
    ensure => present,
    source => "${storagenodeip}:/apps",
    dest   => '/apps',
    type   => 'nfs',
    opts   => 'nofail,defaults,vers=3,rw,noatime',
    require => File['/apps'],
  }

  package { 'torque-scheduler':
    ensure => 'installed',
    source => 'http://web.rcac.purdue.edu/~sharrell/buildacluster/torque-scheduler-4.1.7-1.adaptive.el6.x86_64.rpm',
    provider => 'rpm',
    require => Package['torque']
  }

  # install torque scheduler and resource manager
  package { 'torque-server':
    ensure => 'installed',
    source => 'http://web.rcac.purdue.edu/~sharrell/buildacluster/torque-server-4.1.7-1.adaptive.el6.x86_64.rpm',
    provider => 'rpm',
    require => Package['torque']
  }

  service { "pbs_server":
    ensure  => "running",
    enable  => "true",
    require => Package["torque-server"],
  }

  service { "pbs_sched":
    ensure  => "running",
    enable  => "true",
    require => Package["torque-scheduler"],
  }

  file { '/var/spool/torque/server_priv/nodes':
    content => "compute1.cluster np=1\ncompute2.cluster np=1\n",
    require => Package['torque-server'],
    notify => Service['pbs_server'],
  }

  file { '/var/spool/torque/checkpoint':
    ensure => 'directory',
  }

  # syslog collector for the cluster
  file {'/var/log/multi/':
    ensure => 'directory',
    before => Class['rsyslog::server'],
  }

  class { 'rsyslog::server':
    server_dir => '/var/log/multi/',
  }

}

class storage_node {

  # Create NFS shares on storage node for home and apps
  include nfs::server
  nfs::server::export{ '/home/':
    ensure  => 'mounted',
    clients => '172.31.0.0/16(rw,insecure,async,no_root_squash) localhost(rw)',
  }

  nfs::server::export{ '/apps/':
    ensure  => 'mounted',
    clients => '172.31.0.0/16(rw,insecure,async,no_root_squash) localhost(rw)',
    require => File['/apps']
  }

}

class compute_node {

  # mounts for home and apps NFS share
  mounts { 'storage server home':
    ensure => present,
    source => "${storagenodeip}:/home",
    dest   => '/home',
    type   => 'nfs',
    opts   => 'nofail,defaults,vers=3,rw,noatime',
  }

  mounts { 'storage server apps':
    ensure => present,
    source => "${storagenodeip}:/apps",
    dest   => '/apps',
    type   => 'nfs',
    opts   => 'nofail,defaults,vers=3,rw,noatime',
    require => File['/apps'],
  }

  # Install and setup pbs_mom
  package { 'torque-client':
    ensure => 'installed',
    source => 'http://web.rcac.purdue.edu/~sharrell/buildacluster/torque-client-4.1.7-1.adaptive.el6.x86_64.rpm',
    provider => 'rpm',
    require => Package['torque']
  }

  service { "pbs_mom":
    ensure  => "running",
    enable  => "true",
    require => Package["torque-client"],
  }

  file { '/var/spool/torque/mom_priv/config':
    content => "\$pbsserver head
\$usecp *:/home /home\n
\$node_check_script /usr/sbin/nhc
\$node_check_interval jobstart
\$down_on_error 1\n",
    require => Package['torque-client'],
    notify  => Service["pbs_mom"]
  }

  # install and configure node health checks
  package { 'warewulf-nhc':
    ensure => 'installed',
    source => 'http://warewulf.lbl.gov/downloads/repo/rhel6/warewulf-nhc-1.3-1.el6.noarch.rpm',
    provider => 'rpm',
  }

  file { '/etc/nhc/nhc.conf':
    content => "/./ || check_fs_mount_rw /
 *  || check_ps_daemon sshd root\n
 *  || check_hw_physmem 1024 1073741824\n
 *  || check_hw_physmem_free 1\n",
    require => Package['warewulf-nhc'],
  }

}

# head node
node 'head.cluster', 'ip-172-31-17-193' {
    include head_node
    include base_cluster
}

# storage node
node 'storage.cluster', 'ip-172-31-17-195' {
    include storage_node
    include base_cluster
}

# compute nodes
node  'compute1.cluster', 'compute2.cluster', 'ip-172-31-21-170', 'ip-172-31-17-194' {
    include compute_node
    include base_cluster
}
